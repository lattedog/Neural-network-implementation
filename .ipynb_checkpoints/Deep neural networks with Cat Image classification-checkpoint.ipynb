{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is to realize multi-layer deep neural networks myself, using the cat image data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  Date: 9/14/2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import h5py\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(\"/Users/yuxing/Documents/ML_code/\")\n",
    "\n",
    "# some useful functions\n",
    "import my_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory is:/Users/yuxing/Documents/ML_code/Neural Nets/Cat_Image\n"
     ]
    }
   ],
   "source": [
    "# set up working directory\n",
    "\n",
    "data_dir0 = \"/Users/yuxing/Documents/ML_data/\"\n",
    "code_dir0 = \"/Users/yuxing/Documents/ML_code/Neural Nets/\"\n",
    "\n",
    "project_name = \"Cat_Image/\"\n",
    "\n",
    "data_dir = data_dir0 + project_name\n",
    "code_dir = code_dir0 + project_name\n",
    "\n",
    "output_dir = code_dir\n",
    "\n",
    "if os.getcwd() != code_dir:\n",
    "    os.chdir(code_dir)\n",
    "\n",
    "print(\"Current working directory is:\" + os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'my_lib' from '/Users/yuxing/Documents/ML_code/my_lib.py'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(my_lib)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. load in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_dir):\n",
    "    train_dataset = h5py.File(data_dir + 'train_catvnoncat.h5', \"r\")\n",
    "    train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:]) # your train set features\n",
    "    train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:]) # your train set labels\n",
    "\n",
    "    test_dataset = h5py.File(data_dir + 'test_catvnoncat.h5', \"r\")\n",
    "    test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:]) # your test set features\n",
    "    test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:]) # your test set labels\n",
    "\n",
    "    classes = np.array(test_dataset[\"list_classes\"][:]) # the list of classes\n",
    "    \n",
    "    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n",
    "    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n",
    "    \n",
    "    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes = load_data(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(209, 64, 64, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set_x_orig.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 209)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set_y_orig.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 transfrom the image data 64_64_3 into a long column vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_vector(set_x):\n",
    "    \"\"\"\n",
    "    set_X has 4 dimensions, with the 1st being the number of examples.\n",
    "    The rest of 3 are 64 pixels * 64 pixels * 3 colors (RGB) dimensions.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    vector_x = set_x.reshape(set_x.shape[0], -1).T\n",
    "    \n",
    "    print(\"The shape now is (n_x, m) = \", vector_x.shape)\n",
    "    \n",
    "    return vector_x\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape now is (n_x, m) =  (12288, 209)\n"
     ]
    }
   ],
   "source": [
    "train_set_x = image_to_vector(train_set_x_orig)\n",
    "\n",
    "# n_x is 12288 and we have 209 training examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape now is (n_x, m) =  (12288, 50)\n"
     ]
    }
   ],
   "source": [
    "test_set_x = image_to_vector(test_set_x_orig)\n",
    "\n",
    "# n_x is 12288 and we have 50 training examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2  Pre-processing the data by normalizing it, use the same scaler applied on training data onto the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each row in the data, it's a feature, and we standardize this dimension.\n",
    "\n",
    "# scaler = preprocessing.StandardScaler().fit(train_set_x.T)\n",
    "# scaler = preprocessing.MinMaxScaler().fit(train_set_x.T)\n",
    "\n",
    "# train_set_x2 = (scaler.transform(train_set_x.T)).T \n",
    "# test_set_x2 = (scaler.transform(test_set_x.T)).T   \n",
    "\n",
    "train_set_x2  = train_set_x / 255\n",
    "test_set_x2 = test_set_x / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Build up the helper functions for the neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    \"\"\"\n",
    "    This function computes the sigmoid function for the input matrix.\n",
    "    Output: A[l] = sigmoid(Z[l])\n",
    "    \"\"\"\n",
    "    \n",
    "    return 1 / (1 + np.exp(-Z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_slope(Z):\n",
    "    \"\"\"\n",
    "    This function computes the inverse of the sigmoid function for the input matrix.\n",
    "    Output: sigma * (1 - sigma)\n",
    "    \"\"\"\n",
    "    \n",
    "    sigma = sigmoid(Z)\n",
    "    \n",
    "    return np.multiply(sigma, 1 - sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    \"\"\"\n",
    "    This function computes the rectified linear unit function for the input matrix.\n",
    "    Output: A[l] = np.max(Z, 0)\n",
    "    \"\"\"\n",
    "    return np.maximum(Z, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_slope(Z):\n",
    "    \"\"\"\n",
    "    This function computes the slope of the rectified linear unit function for the input matrix.\n",
    "    Output: as long as the input > 0, the slope is 1; otherwise the slope is 0.\n",
    "    \"\"\"\n",
    "    Z_2 = np.array(Z, copy=True) # just converting dz to a correct object.\n",
    "    Z_2[Z_2 >=0] = 1.0\n",
    "    Z_2[Z_2 < 0] = 0.0\n",
    "    \n",
    "    return Z_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_parameter(X, layers_nums):\n",
    "    \"\"\"\n",
    "    This function initializes the parameters for the W and b matrices. \n",
    "    We use random values following a normal distribution * 0.01 for W and use zeros for b.\n",
    "    \n",
    "    n[l]: number of nodes in layer l, l = 1, 2, 3, ..., L\n",
    "    shape for W[l] = (n[l], n[l-1])\n",
    "    shape for b[l] = (n[l], 1)\n",
    "    \n",
    "    layers_nums doesn't consider the X, it has L-1 hidder layers and 1 output layer.\n",
    "    \n",
    "    The output is a dictionary mapping from the parameter name to its values.\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    \n",
    "    parameter = {}\n",
    "    L = len(layers_nums)\n",
    "    \n",
    "    n_x = X.shape[0]  # the dimension of parameters for X\n",
    "    layers = [n_x] + list(layers_nums)\n",
    "\n",
    "    for l in range(1, L+1):\n",
    "        parameter[\"W\" + str(l)] = np.random.randn(layers[l], layers[l-1]) / np.sqrt(layers[l-1])  ## * 0.01 \n",
    "        parameter[\"b\" + str(l)] = np.zeros((layers[l],1))\n",
    "        \n",
    "    return parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i in range(1, 5 + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(X, parameter, layers_nums, activation = \"relu\"):\n",
    "    \"\"\"\n",
    "    This conducts the forward propagation from the X layer to the final output.\n",
    "    For the hidden layers, we prefer to use relu activations function and for the final output, we \n",
    "    prefer the sigmoid function to produce a probablity in [0,1].\n",
    "    Along the way, Z[l], A[l], W[l] are saved in caches for back_prop use.\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(layers_nums)\n",
    "    \n",
    "    # save some intermedite results for backprop use\n",
    "    caches = {}\n",
    "    \n",
    "    for l in range(1, L+1):\n",
    "        \n",
    "        if l == 1:\n",
    "            Z = np.dot(parameter['W'+ str(l)], X) + parameter['b' + str(l)]\n",
    "            if activation == \"sigmoid\":\n",
    "                A = sigmoid(Z)\n",
    "            if activation == \"relu\":\n",
    "                A = relu(Z)\n",
    "\n",
    "        elif l < L:\n",
    "            \n",
    "            Z = np.dot(parameter['W'+ str(l)], A) + parameter['b' + str(l)]\n",
    "            if activation == \"sigmoid\":\n",
    "                A = sigmoid(Z)\n",
    "            if activation == \"relu\":\n",
    "                A = relu(Z)\n",
    "            \n",
    "        else: # layer L: use sigmoid function\n",
    "            \n",
    "            Z = np.dot(parameter['W'+ str(l)], A) + parameter['b' + str(l)]\n",
    "            A = sigmoid(Z)\n",
    "\n",
    "        caches['Z'+str(l)] = Z\n",
    "        caches['A'+str(l)] = A\n",
    "\n",
    "    return caches['A'+str(L)], caches\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = [7,4,1]\n",
    "parameter = init_parameter(train_set_x2, layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "AL, caches = forward_prop(train_set_x2, parameter, layer, activation = \"relu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(Y, AL):\n",
    "    \"\"\"\n",
    "    Using the activations from the last layer, we calculate the loss function for the current iteration.\n",
    "    \"\"\"\n",
    "    m = Y.shape[1]\n",
    "    cost = (1./m) * (-np.dot(Y,np.log(AL).T) - np.dot(1-Y, np.log(1-AL).T))\n",
    "#     print(cost.shape)\n",
    "    cost = np.squeeze(cost)\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_prop(X, Y, caches, parameter, layers_nums, activation = \"relu\"):\n",
    "    \"\"\"\n",
    "    This function conducts the backward propagation to calculate the gradients for the parameters in NN.\n",
    "    \n",
    "    \"\"\"\n",
    "    L = len(layers_nums)\n",
    "    m = Y.shape[1]\n",
    "    # save the gradients for the parameters.\n",
    "    grads = {}\n",
    "    \n",
    "    AL = caches['A'+str(L)]\n",
    "    \n",
    "    grads['dA'+str(L)]  = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    \n",
    "    for l in range(L,0,-1):\n",
    "        \n",
    "        if l == L: # output layer uses sigmoid function\n",
    "\n",
    "            dZ_l = np.multiply(grads['dA'+str(l)], sigmoid_slope(caches['Z'+str(l)]))\n",
    "            dA_l_1 = np.dot(parameter['W'+str(l)].T, dZ_l)\n",
    "            \n",
    "            dW_l = 1/m * np.dot(dZ_l, caches['A'+str(l-1)].T)\n",
    "            db_l = 1/m * np.sum(dZ_l, axis = 1, keepdims= True)\n",
    "            \n",
    "        elif l > 1:\n",
    "            if activation == 'sigmoid':\n",
    "                dZ_l = np.multiply(grads['dA'+str(l)], sigmoid_slope(caches['Z'+str(l)]))\n",
    "            if activation == 'relu':\n",
    "                dZ_l = np.multiply(grads['dA'+str(l)], relu_slope(caches['Z'+str(l)]))\n",
    "\n",
    "                \n",
    "            dA_l_1 = np.dot(parameter['W'+str(l)].T, dZ_l)\n",
    "            \n",
    "            dW_l = 1/m * np.dot(dZ_l, caches['A'+str(l-1)].T)\n",
    "            db_l = 1/m * np.sum(dZ_l, axis = 1, keepdims= True)\n",
    "            \n",
    "        else: # l == 1\n",
    "            if activation == 'sigmoid':\n",
    "                dZ_l = np.multiply(grads['dA'+str(l)], sigmoid_slope(caches['Z'+str(l)]))\n",
    "            if activation == 'relu':\n",
    "                dZ_l = np.multiply(grads['dA'+str(l)], relu_slope(caches['Z'+str(l)]))\n",
    "\n",
    "                \n",
    "            dA_l_1 = np.dot(parameter['W'+str(l)].T, dZ_l)\n",
    "            \n",
    "            dW_l = 1/m * np.dot(dZ_l, X.T)\n",
    "            db_l = 1/m * np.sum(dZ_l, axis = 1, keepdims= True)\n",
    "            \n",
    "        grads['dA'+str(l-1)] = dA_l_1\n",
    "        grads['dW'+str(l)] = dW_l\n",
    "        grads['db'+str(l)] = db_l\n",
    "    \n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "grads = back_prop(train_set_x2, train_set_y_orig, caches, parameter, layer, activation = \"relu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameter(parameter, grads, learning_rate = 0.05):\n",
    "    \"\"\"\n",
    "    Using the gradient descent to update the parameters.\n",
    "    \"\"\"\n",
    "    \n",
    "    for par in parameter.keys():\n",
    "        parameter[par] = parameter[par] - learning_rate * grads[\"d\" + par]\n",
    "        \n",
    "    return parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_param = update_parameter(parameter, grads, learning_rate = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_nets(X, Y, layers, max_iter = 5000, activation = \"relu\", learning_rate = 0.05, print_error = True):\n",
    "    \"\"\"\n",
    "    This function builds a deep neural network with several steps:\n",
    "    1. initialize the parameters\n",
    "    2. forward propagate the X through the structures\n",
    "    3. compute the loss function\n",
    "    4. backward propagate the cost to calculate the gradients\n",
    "    5. update the parameters using gradient descent; go back to step 2.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    cost = []\n",
    "    \n",
    "    L = len(layers)\n",
    "    \n",
    "#     1. initialize the parameters\n",
    "    parameter = init_parameter(X, layers)\n",
    "    \n",
    "    for i in range(1, max_iter+1):\n",
    "        \n",
    "#       2. forward propagate the X through the structures\n",
    "        AL, caches = forward_prop(X, parameter, layers, activation)\n",
    "    \n",
    "#         print(\"shape of Y\", Y.shape)\n",
    "#         print('shape of AL', caches['A'+str(L)].shape)\n",
    "    \n",
    "#   3. compute the loss function\n",
    "        temp_cost = compute_cost(Y, AL)\n",
    "\n",
    "#     4. backward propagate the cost to calculate the gradients\n",
    "        grads = back_prop(X, Y, caches, parameter, layers, activation)\n",
    "    \n",
    "    \n",
    "#     5. update the parameters using gradient descent; go back to step 2.\n",
    "        parameter = update_parameter(parameter, grads, learning_rate)\n",
    "    \n",
    "    \n",
    "        # print cost function\n",
    "        if print_error == True and i % 100 == 0:\n",
    "            print(\"Round {}, cost function = {}\".format(i, temp_cost))\n",
    "            cost.append(temp_cost)\n",
    "                \n",
    "    # plot the cost\n",
    "\n",
    "    plt.plot(np.squeeze(cost))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per hundreds)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "    return parameter, cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, y, parameter, layers, activation = 'relu'):\n",
    "    \"\"\"\n",
    "    This function is used to predict the results of a  L-layer neural network.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data set of examples you would like to label\n",
    "    parameters -- parameters of the trained model\n",
    "    \n",
    "    Returns:\n",
    "    p -- predictions for the given dataset X\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    n = len(layers) # number of layers in the neural network\n",
    "    p = np.zeros((1,m))\n",
    "    \n",
    "    # Forward propagation\n",
    "    probas, caches = forward_prop(X, parameter, layers, activation)\n",
    "\n",
    "    \n",
    "    # convert probas to 0/1 predictions\n",
    "    for i in range(0, probas.shape[1]):\n",
    "        if probas[0,i] > 0.5:\n",
    "            p[0,i] = 1\n",
    "        else:\n",
    "            p[0,i] = 0\n",
    "    \n",
    "    #print results\n",
    "    #print (\"predictions: \" + str(p))\n",
    "    #print (\"true labels: \" + str(y))\n",
    "    print(\"Accuracy: \"  + str(np.sum((p == y)/m)))\n",
    "        \n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Build the DNN model with data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 100, cost function = 0.6730669087707257\n",
      "Round 200, cost function = 0.6599897603445041\n",
      "Round 300, cost function = 0.6508475169990517\n",
      "Round 400, cost function = 0.6421751895364037\n",
      "Round 500, cost function = 0.6297722175558085\n",
      "Round 600, cost function = 0.6037825154775028\n",
      "Round 700, cost function = 0.5507967572480309\n",
      "Round 800, cost function = 0.47080737525730043\n",
      "Round 900, cost function = 0.3967926375116874\n",
      "Round 1000, cost function = 0.3491790636035602\n",
      "Round 1100, cost function = 0.34054994323010235\n",
      "Round 1200, cost function = 0.2561072208124426\n",
      "Round 1300, cost function = 0.22224474208492456\n",
      "Round 1400, cost function = 0.18929006541750487\n",
      "Round 1500, cost function = 0.16430041799919626\n",
      "Round 1600, cost function = 0.1467225449145274\n",
      "Round 1700, cost function = 0.13562982922449304\n",
      "Round 1800, cost function = 0.12619968987161462\n",
      "Round 1900, cost function = 0.119069220075659\n",
      "Round 2000, cost function = 0.11201332074878421\n",
      "Round 2100, cost function = 0.10632855280017184\n",
      "Round 2200, cost function = 0.10134832418876939\n",
      "Round 2300, cost function = 0.09742499838996015\n",
      "Round 2400, cost function = 0.09352251739716107\n",
      "Round 2500, cost function = 0.0900996071929103\n",
      "Round 2600, cost function = 0.08725409970105959\n",
      "Round 2700, cost function = 0.08465322769778594\n",
      "Round 2800, cost function = 0.08227380419010327\n",
      "Round 2900, cost function = 0.08009937697787611\n",
      "Round 3000, cost function = 0.07824507299877259\n",
      "Round 3100, cost function = 0.07648218531108456\n",
      "Round 3200, cost function = 0.07489668627980124\n",
      "Round 3300, cost function = 0.07341723795607936\n",
      "Round 3400, cost function = 0.07227349324782398\n",
      "Round 3500, cost function = 0.07088446013197068\n",
      "Round 3600, cost function = 0.06977633477710551\n",
      "Round 3700, cost function = 0.068994447313864\n",
      "Round 3800, cost function = 0.06778673968974892\n",
      "Round 3900, cost function = 0.06725389490224126\n",
      "Round 4000, cost function = 0.06604467750907482\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9904306220095692\n",
      "Accuracy: 0.8200000000000001\n"
     ]
    }
   ],
   "source": [
    "layers = [20, 7, 5, 3, 1]\n",
    "\n",
    "parameter, cost = neural_nets(train_set_x2, train_set_y_orig, layers, max_iter = 4000, \n",
    "                                activation = \"relu\", learning_rate = 0.01)\n",
    "\n",
    "predictions_train = predict(train_set_x2, train_set_y_orig,  parameter, layers, activation = \"relu\")\n",
    "\n",
    "predictions_test = predict(test_set_x2, test_set_y_orig,  parameter, layers, activation = \"relu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
